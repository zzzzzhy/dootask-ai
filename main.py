from types import SimpleNamespace
from pathlib import Path
from fastapi import FastAPI, Request, Header
from fastapi.concurrency import asynccontextmanager
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, StreamingResponse, FileResponse
from helper.utils import convert_message_content_to_string, dict_to_message, get_model_instance, get_swagger_ui, json_empty, json_error, json_content, message_to_dict, remove_tool_calls, replace_think_content, remove_reasoning_content, process_html_content
from helper.request import RequestClient
from helper.invoke import parse_context, build_invoke_stream_key
from helper.redis import handle_context_limits, RedisManager
from helper.config import SERVER_PORT, CLEAR_COMMANDS, STREAM_TIMEOUT, END_CONVERSATION_MARK
import json
import time
import random
import string
import httpx
from langchain_mcp_adapters.client import MultiServerMCPClient
import asyncio
from exceptiongroup import ExceptionGroup
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, AIMessageChunk

from langchain.agents import create_agent 
from helper.models import ModelListError, get_models_list
import logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler()  # è¾“å‡ºåˆ°æ§åˆ¶å°
    ]
)
logger = logging.getLogger("ai")


UI_DIST_PATH = Path(__file__).resolve().parent / "static" / "ui"


def ui_assets_available() -> bool:
    return UI_DIST_PATH.exists() and UI_DIST_PATH.is_dir()

async def check_website_async(app: FastAPI):
    """æ£€æµ‹MCPæ˜¯å¦å®‰è£…"""
    url = "http://nginx/apps/mcp_server/healthz"  # æ›¿æ¢ä¸ºä½ çš„ç½‘å€
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get( url, timeout=3 )
            if response.json().get("status") == "ok":
                app.state.mcp = True
            else:
                app.state.mcp = False
    except Exception as e:
        app.state.mcp = False
        logger.error(f"âŒ æ£€æµ‹MCPå¤±è´¥: {url} - é”™è¯¯: {e}")

async def periodic_check(app: FastAPI):
    """å®šæ—¶æ£€æµ‹ä»»åŠ¡"""
    while True:
        await check_website_async(app)
        await asyncio.sleep(60)  # 10åˆ†é’Ÿ = 600ç§’

@asynccontextmanager
async def lifespan(app: FastAPI):
    # å¯åŠ¨æ—¶åˆå§‹åŒ–
    try:
        task = asyncio.create_task(periodic_check(app))
        redis_manager = RedisManager()
        logger.info("âœ… åˆå§‹åŒ–æˆåŠŸ")
        app.state.redis_manager = redis_manager
    except Exception as e:
        logger.info(f"âŒ åˆå§‹åŒ–å¤±è´¥: {str(e)}")
    yield
    # å…³é—­æ—¶
    task.cancel()
    try:
        await task
    except asyncio.CancelledError:
        pass
    logger.info("âœ… å®šæ—¶ä»»åŠ¡å·²åœæ­¢")
    # å…³é—­æ—¶æ¸…ç†
    logger.info("ğŸ›‘ AIæœåŠ¡æ­£åœ¨å…³é—­...")

app = FastAPI(
    title="AI Chat API",
    description="åŸºäºAIçš„èŠå¤©æœåŠ¡API",
    version="1.0.0",
    lifespan=lifespan
)
# é…ç½®CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.api_route("/chat", methods=["GET", "POST"])
async def chat(request: Request):
    # æ™ºèƒ½å‚æ•°æå–
    if request.method == "GET":
        params = dict(request.query_params)
    else:
        form_data = await request.form()
        params = dict(form_data)

    # å‚æ•°é…ç½®
    defaults = {
        'dialog_id': 0,
        'msg_id': 0,
        'msg_uid': 0,
        'mention': 0,
        'bot_uid': 0,
        'extras': '{}'
    }
    
    # åº”ç”¨é»˜è®¤å€¼å’Œç±»å‹è½¬æ¢
    for key, default_value in defaults.items():
        value = params.get(key, default_value)
        if isinstance(default_value, int):
            try:
                params[key] = int(value)
            except (ValueError, TypeError):
                params[key] = default_value
        else:
            params[key] = value
    
    text = params.get("text")
    token = params.get("token")
    version = params.get("version")
    dialog_id, msg_id, msg_uid, mention, bot_uid, extras = (
        params[k] for k in ["dialog_id", "msg_id", "msg_uid", "mention", "bot_uid", "extras"]
    )

    # æ£€æŸ¥å¿…è¦å‚æ•°æ˜¯å¦ä¸ºç©º
    if not all([text, token, dialog_id, msg_uid, bot_uid, version]):
        return JSONResponse(content={"code": 400, "error": "Parameter error"}, status_code=200)

    # è§£æ extras å‚æ•°
    try:
        extras_json = json.loads(extras)
        model_type = extras_json.get('model_type', 'openai')
        model_name = extras_json.get('model_name', 'gpt-5-nano')
        system_message = extras_json.get('system_message')
        server_url = extras_json.get('server_url')
        api_key = extras_json.get('api_key')
        base_url = extras_json.get('base_url')
        agency = extras_json.get('agency')
        temperature = float(extras_json.get('temperature', 0.7))
        max_tokens = int(extras_json.get('max_tokens', 0))
        thinking = int(extras_json.get('thinking', 0))
        before_text = extras_json.get('before_text')
        before_clear = int(extras_json.get('before_clear', 0))
        context_key = extras_json.get('context_key', '')
        context_limit = int(extras_json.get('context_limit', 0))
    except json.JSONDecodeError:
        return JSONResponse(content={"code": 400, "error": "Invalid extras parameter"}, status_code=200)

    # æ£€æŸ¥ extras è§£æåçš„å¿…è¦å‚æ•°æ˜¯å¦ä¸ºç©º
    if not all([model_type, model_name, server_url, api_key]):
        return JSONResponse(content={"code": 400, "error": "Parameter error in extras"}, status_code=200)

    # ä¸Šä¸‹æ–‡ before_text å¤„ç†
    if not before_text:
        before_text = []
    elif isinstance(before_text, str):
        before_text = [HumanMessage(content=before_text)]
    elif isinstance(before_text, list):
        if before_text and isinstance(before_text[0], str):
            before_text = [HumanMessage(content=text) for text in before_text]

    # åˆ›å»ºè¯·æ±‚å®¢æˆ·ç«¯
    request_client = RequestClient(server_url, version, token, dialog_id)

    # å®šä¹‰ä¸Šä¸‹æ–‡é”®
    context_key = f"{model_type}_{model_name}_{dialog_id}_{context_key}"
    
    # å¦‚æœæ˜¯æ¸…ç©ºä¸Šä¸‹æ–‡çš„å‘½ä»¤
    if text in CLEAR_COMMANDS:
        await app.state.redis_manager.delete_context(context_key)
        # è°ƒç”¨å›è°ƒ
        asyncio.ensure_future(request_client.call({
            "notice": "ä¸Šä¸‹æ–‡å·²æ¸…ç©º",
            "silence": "yes",
            "source": "ai",
        }, action='notice'))
        return JSONResponse(content={"code": 200, "data": {"desc": "Context cleared"}}, status_code=200)

    # å¦‚æœéœ€è¦åœ¨è¯·æ±‚å‰æ¸…ç©ºä¸Šä¸‹æ–‡
    if before_clear:
        await app.state.redis_manager.delete_context(context_key)

    # åˆ›å»ºæ¶ˆæ¯
    send_id = await request_client.call({
        "text": '...',
        "text_type": "md",
        "silence": "yes",
        "reply_id": msg_id,
        "reply_check": "yes",
    })
    
    if not send_id:
        return JSONResponse(content={"code": 400, "error": "Send message failed"}, status_code=200)

    # å¤„ç†HTMLå†…å®¹ï¼ˆå›¾ç‰‡æ ‡ç­¾ï¼‰
    text = process_html_content(text)

    # ç”Ÿæˆéšæœº8ä½å­—ç¬¦ä¸²
    stream_key = ''.join(random.choices(string.ascii_letters + string.digits, k=8))
    
    # å°†è¾“å…¥å­˜å‚¨åˆ° Redis
    await app.state.redis_manager.set_input(send_id, {
        "text": text,
        "token": token,
        "dialog_id": dialog_id,
        "version": version,
        "msg_user_token": params.get("msg_user[token]"),
        "before_text": before_text,
        "model_type": model_type,
        "model_name": model_name,
        "system_message": system_message,
        "server_url": server_url,
        "api_key": api_key,
        "base_url": base_url,
        "agency": agency,
        "temperature": temperature,
        "max_tokens": max_tokens,
        "thinking": thinking,
        "context_limit": context_limit,

        "context_key": context_key,
        "stream_key": stream_key,
        "created_at": int(time.time()),
        "status": "prepare",
        "response": "",
    })

    # é€šçŸ¥ stream åœ°å€
    asyncio.create_task(request_client.call({
        "userid": msg_uid,
        "stream_url": f"/stream/{send_id}/{stream_key}",
        "source": "ai",
    }, action='stream'))

    # è¿”å›æˆåŠŸå“åº”
    return JSONResponse(content={"code": 200, "data": {"id": send_id, "key": stream_key}}, status_code=200)

# å¤„ç†æµå¼å“åº”
@app.get('/stream/{msg_id}/{stream_key}')
async def stream(msg_id: str, stream_key: str, host: str = Header("", alias="Host"), scheme: str = Header("http", alias="scheme")):
    if not stream_key:
        async def error_stream():
            yield f"id: {msg_id}\nevent: done\ndata: {json_error('No key')}\n\n"
        return StreamingResponse(
            error_stream(),
            media_type='text/event-stream'
        )

    # æ£€æŸ¥ msg_id æ˜¯å¦åœ¨ Redis ä¸­
    data = await app.state.redis_manager.get_input(msg_id)

    if not data:
        async def error_stream():
            yield f"id: {msg_id}\nevent: done\ndata: {json_error('No such ID')}\n\n"
        return StreamingResponse(
            error_stream(),
            media_type='text/event-stream'
        )

    # æ£€æŸ¥ stream_key æ˜¯å¦æ­£ç¡®
    if stream_key != data["stream_key"]:
        async def error_stream():
            yield f"id: {msg_id}\nevent: done\ndata: {json_error('Invalid key')}\n\n"
        return StreamingResponse(
            error_stream(),
            media_type='text/event-stream'
        )

    # å¦‚æœ status ä¸º finishedï¼Œç›´æ¥è¿”å›
    if data["status"] == "finished":
        async def finished_stream():
            yield f"""
            id: {msg_id}\nevent: replace\ndata: {json_content(data['response'])}\n\n
            id: {msg_id}\nevent: done\ndata: {json_empty()}\n\n
            """
        return StreamingResponse(
            finished_stream(),
            media_type='text/event-stream'
        )
    tools = []
    if app.state.mcp:
        client = MultiServerMCPClient(
            {
                "dootask-task": {
                    "url": f"{scheme}://{host}/apps/mcp_server/mcp",
                    "transport": "streamable_http",
                    "headers": {
                        "token": data.get("msg_user_token","unknown")
                    },
                }
            }
        )
        tools = await client.get_tools()
    async def stream_generate(msg_id, msg_key, data, redis_manager):
        """
        æµå¼ç”Ÿæˆå“åº”
        """

        response = ""
        try:
            # æ›´æ–°æ•°æ®çŠ¶æ€
            data["status"] = "processing"
            await redis_manager.set_input(msg_id, data)
            # è·å–å¯¹åº”çš„æ¨¡å‹å®ä¾‹
            model = get_model_instance(
                model_type=data["model_type"],
                model_name=data["model_name"],
                api_key=data["api_key"],
                base_url=data["base_url"],
                agency=data["agency"],
                temperature=data["temperature"],
                max_tokens=data["max_tokens"],
                thinking=data["thinking"],
                streaming=True,
            )

            # å‰ç½®ä¸Šä¸‹æ–‡å¤„ç†
            pre_context = []

            # æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯åˆ°ä¸Šä¸‹æ–‡å¼€å§‹
            if data["system_message"]:
                pre_context.append(SystemMessage(content=data["system_message"]))

            # æ·»åŠ  before_text åˆ°ä¸Šä¸‹æ–‡
            if data["before_text"]:
                # è¿™äº›æ¨¡å‹ä¸æ”¯æŒè¿ç»­çš„æ¶ˆæ¯ï¼Œéœ€è¦åœ¨æ¯æ¡æ¶ˆæ¯ä¹‹é—´æ’å…¥ç¡®è®¤æ¶ˆæ¯
                models_need_confirmation = ["deepseek-reasoner", "deepseek-coder"]
                if data["model_name"] in models_need_confirmation:
                    for msg in data["before_text"]:
                        pre_context.append(msg)
                        pre_context.append(AIMessage(content="å¥½çš„ï¼Œæ˜ç™½äº†ã€‚"))
                else:
                    pre_context.extend(data["before_text"])

            # è·å–ç°æœ‰ä¸Šä¸‹æ–‡
            middle_context = await redis_manager.get_context(data["context_key"])

            middle_messages = []
            if middle_context:
                middle_messages = [dict_to_message(msg_dict) for msg_dict in middle_context]
            # æ·»åŠ ç”¨æˆ·çš„æ–°æ¶ˆæ¯
            end_context = [HumanMessage(content=data["text"])]
            # å¤„ç†æ¨¡å‹é™åˆ¶
            final_context = handle_context_limits(
                pre_context=pre_context,
                middle_context=middle_messages,
                end_context=end_context,
                model_type=data["model_type"], 
                model_name=data["model_name"], 
                custom_limit=data["context_limit"]
            )
            # æ£€æŸ¥ä¸Šä¸‹æ–‡æ˜¯å¦è¶…é™
            if not final_context:
                raise Exception("Context limit exceeded")
            # ç¼“å­˜é…ç½®
            cache_interval = 0.1  # ç¼“å­˜é—´éš”
            last_cache_time = time.time()
            # çŠ¶æ€å˜é‡
            has_reasoning = False
            is_response = False
            
            agent = create_agent(model, tools)
            
            # å¼€å§‹è¯·æ±‚æµå¼å“åº”
            async for chunk in agent.astream({"messages": final_context}, stream_mode="messages"):
                # logger.info(chunk)
                msg, metadata = chunk
                if "skip_stream" in metadata.get("tags", []):
                    continue
                # For some reason, astream("messages") causes non-LLM nodes to send extra messages.
                # Drop them.
                if not isinstance(msg, AIMessageChunk):
                    continue

                if hasattr(msg, 'content') and isinstance(msg.content, list):
                    isContinue = True
                    if msg.content:
                        chunk = SimpleNamespace(**msg.content[0])
                        if hasattr(chunk, 'type'):
                            if chunk.type == 'thinking' and hasattr(chunk, 'thinking'):    
                                chunk = SimpleNamespace(reasoning_content=chunk.thinking)
                                isContinue = False
                            elif chunk.type == 'reasoning' and hasattr(chunk, 'reasoning'):    
                                chunk = SimpleNamespace(reasoning_content=chunk.reasoning)
                                isContinue = False
                            elif chunk.type == 'text' and hasattr(chunk, 'text'):
                                chunk = SimpleNamespace(content=chunk.text)
                                isContinue = False
                    if isContinue:
                        continue

                if hasattr(msg, 'reasoning_content') and msg.reasoning_content and not is_response:
                    if not has_reasoning:
                        response += "::: reasoning\n"
                        has_reasoning = True
                    response += convert_message_content_to_string(msg.reasoning_content)
                    response = replace_think_content(response)
                    current_time = time.time()
                    if current_time - last_cache_time >= cache_interval:
                        await redis_manager.set_cache(msg_key, response, ex=STREAM_TIMEOUT)
                        last_cache_time = current_time  
                        
                if hasattr(msg, 'content') and msg.content:
                    if has_reasoning:
                        response += "\n:::\n\n"
                        has_reasoning = False
                    is_response = True
                    response += convert_message_content_to_string(remove_tool_calls(msg.content))
                    response = replace_think_content(response)
                    current_time = time.time()
                    if current_time - last_cache_time >= cache_interval:
                        await redis_manager.set_cache(msg_key, response, ex=STREAM_TIMEOUT)
                        last_cache_time = current_time                    

            # æ›´æ–°ä¸Šä¸‹æ–‡
            if response:    
                await redis_manager.extend_contexts(data["context_key"], [
                    message_to_dict(HumanMessage(content=data["text"])),
                    message_to_dict(AIMessage(content=remove_reasoning_content(response)))
                ], data["model_type"], data["model_name"], data["context_limit"])

        except Exception as e:
            # å¤„ç†å¼‚å¸¸
            logger.exception(e)
            response = str(e)
        finally:
            # ç¡®ä¿çŠ¶æ€æ€»æ˜¯è¢«æ›´æ–°
            try:
                # æ›´æ–°å®Œæ•´ç¼“å­˜
                await redis_manager.set_cache(msg_key, response, ex=STREAM_TIMEOUT)

                # æ›´æ–°æ•°æ®çŠ¶æ€
                data["status"] = "finished"
                data["response"] = response
                await redis_manager.set_input(msg_id, data)

                # åˆ›å»ºè¯·æ±‚å®¢æˆ·ç«¯
                request_client = RequestClient(
                    server_url=data["server_url"], 
                    version=data["version"], 
                    token=data["token"], 
                    dialog_id=data["dialog_id"]
                )

                # æ›´æ–°å®Œæ•´æ¶ˆæ¯
                asyncio.ensure_future(request_client.call({
                    "update_id": msg_id,
                    "update_mark": "no",
                    "text": response,
                    "text_type": "md",
                    "silence": "yes"
                }))
            except Exception as e:
                # è®°å½•æœ€ç»ˆé˜¶æ®µçš„é”™è¯¯ï¼Œä½†ä¸å½±å“ä¸»æµç¨‹
                logger.error(f"Error in cleanup: {str(e)}")

    async def stream_producer():
        """
        æµå¼ç”Ÿäº§è€…
        """

        # ç”Ÿæˆæ¶ˆæ¯ key
        msg_key = f"stream_msg_{msg_id}"
        producer_task = None
        
        # å¦‚æœæ˜¯ç¬¬ä¸€ä¸ªè¯·æ±‚ï¼Œå¯åŠ¨å¼‚æ­¥ç”Ÿäº§è€…
        if await app.state.redis_manager.set_cache(msg_key, "", ex=STREAM_TIMEOUT, nx=True):
            producer_task = asyncio.create_task(stream_generate(msg_id, msg_key, data, app.state.redis_manager))

        # æ‰€æœ‰è¯·æ±‚éƒ½ä½œä¸ºæ¶ˆè´¹è€…å¤„ç†
        wait_start = time.time()
        last_response = ""
        sleep_interval = 0.1  # ç¡çœ é—´éš”
        timeout_check_interval = 1.0  # æ£€æŸ¥è¶…æ—¶é—´éš”
        last_timeout_check = time.time()
        check_status_interval = 0.2  # æ£€æŸ¥å®ŒæˆçŠ¶æ€é—´éš”
        last_status_check = time.time()
        
        while True:
            current_time = time.time()
            
            # æ£€æŸ¥è¶…æ—¶
            if current_time - last_timeout_check >= timeout_check_interval:
                if current_time - wait_start > STREAM_TIMEOUT:
                    yield f"id: {msg_id}\nevent: replace\ndata: {json_content('Request timeout')}\n\n"
                    yield f"id: {msg_id}\nevent: done\ndata: {json_error('Timeout')}\n\n"
                    if producer_task:
                        producer_task.cancel()
                    return
                last_timeout_check = current_time

            response = await app.state.redis_manager.get_cache(msg_key)
            if response:
                if not last_response:
                    yield f"id: {msg_id}\nevent: replace\ndata: {json_content(response)}\n\n"
                else:
                    append_response = response[len(last_response):]
                    if append_response:
                        yield f"id: {msg_id}\nevent: append\ndata: {json_content(append_response)}\n\n"
                last_response = response

                # åªåœ¨æœ‰æ–°å“åº”æ—¶æ‰æ£€æŸ¥çŠ¶æ€
                if current_time - last_status_check >= check_status_interval:
                    current_data = await app.state.redis_manager.get_input(msg_id)
                    if current_data and current_data["status"] == "finished":
                        yield f"id: {msg_id}\nevent: done\ndata: {json_empty()}\n\n"
                        if producer_task:
                            producer_task.cancel()
                        return
                    last_status_check = current_time

            # ç¡çœ ç­‰å¾…
            await asyncio.sleep(sleep_interval)

    # è¿”å›æµå¼å“åº”
    return StreamingResponse(
        stream_producer(),
        media_type='text/event-stream'
    )

# ç›´è¿æ¨¡å‹ï¼šæäº¤å‚æ•°ç”Ÿæˆ stream_keyï¼Œå†ç”¨ SSE GET è·å–å“åº”
@app.post('/invoke/auth')
@app.get('/invoke/auth')
async def invoke_auth(request: Request, token: str = Header(..., alias="Authorization")):
    """
    åˆ›å»ºç›´è¿æµè¯·æ±‚å¹¶è¿”å›å¯ä¾› SSE è¿æ¥çš„ stream_keyã€‚
    """
    if request.method == "GET":
        params = dict(request.query_params)
    else:
        form_data = await request.form()
        params = dict(form_data)
    defaults = {
        'model_type': 'openai',
        'model_name': 'gpt-5-chat',
        'max_tokens': 0,
        'temperature': 0.7,
        'thinking': 0,
    }
    
    # åº”ç”¨é»˜è®¤å€¼å’Œç±»å‹è½¬æ¢
    for key, default_value in defaults.items():
        value = params.get(key, default_value)
        if isinstance(default_value, int):
            try:
                params[key] = int(value)
            except (ValueError, TypeError):
                params[key] = default_value
        else:
            params[key] = value
    
    context_messages = parse_context(params.get("context"))
    logger.info(f"Context messages: {context_messages}")
    api_key = params.get('api_key')
    base_url = params.get('base_url')
    agency = params.get('agency')

    model_type, model_name, max_tokens, temperature, thinking = (
        params[k] for k in defaults.keys()
    )

    # æ£€æŸ¥å¿…è¦å‚æ•°æ˜¯å¦ä¸ºç©º
    if not all([context_messages, api_key]):
        return JSONResponse(content={"code": 400, "error": "Parameter error"}, status_code=200)
    
    stream_key = ''.join(random.choices(string.ascii_letters + string.digits, k=12))
    storage_key = build_invoke_stream_key(stream_key)

    await app.state.redis_manager.set_input(storage_key, {
        "final_context": [message_to_dict(content) for content in context_messages],
        "model_type": model_type,
        "model_name": model_name,
        "api_key": api_key,
        "base_url": base_url,
        "user_token": token,
        "agency": agency,
        "temperature": temperature,
        "max_tokens": max_tokens,
        "thinking": thinking,
        "status": "pending",
        "response": "",
        "created_at": int(time.time()),
    })

    return JSONResponse(
        content={
        "code": 200, 
        "data": {
            "stream_key": stream_key,
            "stream_url": f"/invoke/stream/{stream_key}"
        }
    })

# å¤„ç†ç›´æ¥è¯·æ±‚
@app.post('/invoke/stream/{stream_key}')
@app.get('/invoke/stream/{stream_key}')
async def invoke(request: Request, stream_key: str):
    if not stream_key:
        async def error_stream():
            yield f"id: {stream}\nevent: done\ndata: {json_error('No key')}\n\n"
        return StreamingResponse(
            error_stream(),
            media_type='text/event-stream'
        )
    storage_key = build_invoke_stream_key(stream_key)
    # æ£€æŸ¥ msg_id æ˜¯å¦åœ¨ Redis ä¸­
    data = await app.state.redis_manager.get_input(storage_key)
    if not data:
        async def error_stream():
            yield f"id: {stream_key}\nevent: done\ndata: {json_error('No such ID')}\n\n"
        return StreamingResponse(
            error_stream(),
            media_type='text/event-stream'
        )


    # å¦‚æœ status ä¸º finishedï¼Œç›´æ¥è¿”å›
    if data["status"] == "finished" and data.get("response"):
        async def finished_stream():
            yield f"""
            id: {stream_key}\nevent: replace\ndata: {json_content(data['response'])}\n\n
            id: {stream_key}\nevent: done\ndata: {json_empty()}\n\n
            """
        return StreamingResponse(
            finished_stream(),
            media_type='text/event-stream'
        )    
    
    if data.get("status") == "processing":
        async def processing_stream():
            yield f"id: {stream_key}\nevent: done\ndata: {json_error('Stream is processing')}\n\n"
        return StreamingResponse(
            processing_stream(),
            media_type='text/event-stream'
        )    

    stored_context = data.get("final_context") or []
    final_context = parse_context(stored_context)
    if not final_context:
        async def no_context_stream():
            yield f"id: {stream_key}\nevent: done\ndata: {json_error('No context found')}\n\n"
        return StreamingResponse(
            no_context_stream(),
            media_type='text/event-stream'
        )
    
    try:

        model = get_model_instance(
            model_type=data["model_type"],
            model_name=data["model_name"],
            api_key=data["api_key"],
            base_url=data["base_url"],
            agency=data["agency"],
            temperature=data["temperature"],
            max_tokens=data["max_tokens"],
            thinking=data["thinking"],
            streaming=True,
        )
        host = request.headers.get("Host")
        tools = []
        if app.state.mcp:
            client = MultiServerMCPClient(
                {
                    "dootask-task": {
                        "url": f"https://{host}/apps/mcp_server/mcp",
                        "transport": "streamable_http",
                        "headers": {
                            "token": data.get("user_token","unknown")
                        },
                    }
                }
            )
            tools = await client.get_tools()
        agent = create_agent(model, tools)

    except Exception as exc:
        async def model_error_stream():
            yield f"id: {stream_key}\nevent: done\ndata: {json_error(str(exc))}\n\n"
        return StreamingResponse(
            model_error_stream(),
            media_type='text/event-stream'
        )

    async def stream_invoke_response():
        response_text = ""
        last_sent = ""
        has_reasoning = False
        is_response = False
        data["status"] = "processing"
        await app.state.redis_manager.set_input(storage_key, data)
        try:
            async for chunk in agent.astream({"messages": final_context}, stream_mode="messages"):
                # logger.info(chunk)
                msg, metadata = chunk
                if "skip_stream" in metadata.get("tags", []):
                    continue
                # For some reason, astream("messages") causes non-LLM nodes to send extra messages.
                # Drop them.
                if not isinstance(msg, AIMessageChunk):
                    continue
                if hasattr(chunk, "content") and isinstance(msg.content, list):
                    should_continue = True
                    if msg.content:
                        chunk = SimpleNamespace(**msg.content[0])
                        if hasattr(chunk, "type"):
                            if chunk.type == "thinking" and hasattr(chunk, "thinking"):
                                chunk = SimpleNamespace(reasoning_content=chunk.thinking)
                                should_continue = False
                            elif chunk.type == "reasoning" and hasattr(chunk, "reasoning"):
                                chunk = SimpleNamespace(reasoning_content=chunk.reasoning)
                                should_continue = False
                            elif chunk.type == "text" and hasattr(chunk, "text"):
                                chunk = SimpleNamespace(content=chunk.text)
                                should_continue = False
                    if should_continue:
                        continue

                if hasattr(msg, "reasoning_content") and msg.reasoning_content and not is_response:
                    if not has_reasoning:
                        response_text += "::: reasoning\n"
                        has_reasoning = True
                    response_text += msg.reasoning_content
                    response_text = replace_think_content(response_text)
                if hasattr(msg, "content") and msg.content:
                    if has_reasoning:
                        response_text += "\n:::\n\n"
                        has_reasoning = False
                    is_response = True
                    response_text += msg.content
                    response_text = replace_think_content(response_text)

                if response_text != last_sent:
                    if last_sent and response_text.startswith(last_sent):
                        delta = response_text[len(last_sent):]
                        event_type = "append"
                    else:
                        delta = response_text
                        event_type = "replace"
                    if delta:
                        yield f"id: {stream_key}\nevent: {event_type}\ndata: {json_content(delta)}\n\n"
                    last_sent = response_text

            data["status"] = "finished"
            data["response"] = response_text
            app.state.redis_manager.set_input(storage_key, data)
            yield f"id: {stream_key}\nevent: done\ndata: {json_empty()}\n\n"
        except Exception as exc:
            data["status"] = "finished"
            data["response"] = response_text or str(exc)
            data["error"] = str(exc)
            app.state.redis_manager.set_input(storage_key, data)
            yield f"id: {stream_key}\nevent: done\ndata: {json_error(str(exc))}\n\n"
    return StreamingResponse(
        stream_invoke_response(),
        media_type='text/event-stream'
    )
    
# ç›´è¿æ¨¡å‹ï¼šåŒæ­¥è¿”å›å®Œæ•´å“åº”ï¼Œä¸ä½¿ç”¨æµå¼è¾“å‡ºã€‚
@app.api_route('/invoke/synch', methods=['POST', 'GET'])
async def invoke_synch(request: Request, token: str = Header(..., alias="Authorization")):
    """
    ç›´è¿æ¨¡å‹ï¼šåŒæ­¥è¿”å›å®Œæ•´å“åº”ï¼Œä¸ä½¿ç”¨æµå¼è¾“å‡ºã€‚
    """
    if request.method == "GET":
        params = dict(request.query_params)
    else:
        form_data = await request.form()
        params = dict(form_data)
    defaults = {
        'model_type': 'openai',
        'model_name': 'gpt-5-chat',
        'max_tokens': 0,
        'temperature': 0.7,
        'thinking': 0,
    }
    
    # åº”ç”¨é»˜è®¤å€¼å’Œç±»å‹è½¬æ¢
    for key, default_value in defaults.items():
        value = params.get(key, default_value)
        if isinstance(default_value, int):
            try:
                params[key] = int(value)
            except (ValueError, TypeError):
                params[key] = default_value
        else:
            params[key] = value

    context_messages = parse_context(params.get("context"))

    api_key = params.get('api_key')
    base_url = params.get('base_url')
    agency = params.get('agency')

    model_type, model_name, max_tokens, temperature, thinking = (
        params[k] for k in defaults.keys()
    )

    # æ£€æŸ¥å¿…è¦å‚æ•°æ˜¯å¦ä¸ºç©º
    if not all([context_messages, api_key]):
        return JSONResponse(content={"code": 400, "error": "Parameter error"}, status_code=200)
    
    try:
        model = get_model_instance(
            model_type=model_type,
            model_name=model_name,
            api_key=api_key,
            base_url=base_url,
            agency=agency,
            temperature=temperature,
            max_tokens=max_tokens,
            thinking=thinking,
            streaming=False,
        )
        host = request.headers.get("Host")
        tools = []
        if app.state.mcp:
            client = MultiServerMCPClient(
                {
                    "dootask-task": {
                        "url": f"https://{host}/apps/mcp_server/mcp",
                        "transport": "streamable_http",
                        "headers": {
                            "token": token or "unknown"
                        },
                    }
                }
            )
            tools = await client.get_tools()
        agent = create_agent(model, tools)
    except Exception as exc:
        return JSONResponse(content={"code": 400, "error": str(exc)}, status_code=400)

    try:
        logger.info(context_messages)
        result = await agent.ainvoke({"messages": context_messages})
        response_text = result["messages"][-1].content
        response_text = replace_think_content(response_text)
        response_text = remove_reasoning_content(response_text)
        return JSONResponse(content={"code": 200, "data": {"content": response_text}}, status_code=200)
    except Exception as exc:
        return JSONResponse(content={"code": 500, "error": str(exc)}, status_code=500)


# å‰ç«¯ UI é¦–é¡µè·¯ç”±
@app.get('/')
async def root():
    if not ui_assets_available():
        return JSONResponse(content={"message": "DooTask AI service"}, status_code=200)
    return FileResponse(UI_DIST_PATH / 'index.html')

# å‰ç«¯ UI é™æ€èµ„æºè·¯ç”±
@app.get('/ui/')
@app.get('/ui/{path:path}')
async def ui_assets(path: str = 'index.html'):
    if not ui_assets_available():
        return JSONResponse(content={"error": "UI assets not available"}, status_code=404)

    safe_path = path.lstrip("/")
    target = UI_DIST_PATH / safe_path
    if target.exists() and target.is_file():
        return FileResponse(target)

    return FileResponse(UI_DIST_PATH / 'index.html')

# è·å–æ¨¡å‹åˆ—è¡¨
@app.get('/models/list')
async def models_list(
    type: str = '',
    base_url: str = '',
    key: str = '',
    agency: str = ''
):
    model_type = type.strip()
    base_url = base_url.strip()
    key = key.strip()
    agency = agency.strip()

    try:
        data = get_models_list(
            model_type,
            base_url=base_url or None,
            key=key or None,
            agency=agency or None,
        )
    except ModelListError as exc:
        return JSONResponse(content={"code": 400, "error": str(exc)}, status_code=400)
    except Exception as exc:  # pragma: no cover - defensive logging
        import logging
        logger = logging.getLogger(__name__)
        logger.exception("Failed to get model list")
        return JSONResponse(content={"code": 500, "error": "è·å–å¤±è´¥"}, status_code=500)

    return JSONResponse(content={"code": 200, "data": data}, status_code=200)

# å¥åº·æ£€æŸ¥
@app.get('/health')
async def health():
    try:

        await app.state.redis_manager.client.ping()
        return JSONResponse(content={"status": "healthy", "redis": "connected"}, status_code=200)
    except Exception as e:
        return JSONResponse(content={"status": "unhealthy", "error": str(e)}, status_code=500)


# Swagger UI route
@app.get('/swagger')
async def swagger():
    return get_swagger_ui()

# Swagger YAML route
@app.get('/swagger.yaml')
async def swagger_yaml():
    static_file_path = Path(__file__).resolve().parent / "static" / "swagger.yaml"
    return FileResponse(static_file_path)

if __name__ == '__main__':
    import uvicorn
    uvicorn.run("main:app", host='0.0.0.0', port=8080,reload=True)
